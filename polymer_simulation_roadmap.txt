Title: Modeling and Simulations of Polymers: A Roadmap
Abstract: Molecular modeling and simulations are invaluable tools for the polymer science and engineering community. These computational approaches enable predictions and provide explanations of experimentally observed macromolecular structure, dynamics, thermodynamics, and microscopic and macroscopic material properties. With recent advances in computing power, polymer simulations can synergistically inform, guide, and complement in vitro macromolecular materials design and discovery efforts. To ensure that this growing power of simulations is harnessed correctly, and meaningful results are achieved, care must be taken to ensure the validity and reproducibility of these simulations. With these considerations in mind, in this Perspective we discuss our philosophy for carefully developing or selecting appropriate models, performing, and analyzing polymer simulations. We highlight best practices, key challenges, and important advances in model development/selection, computational method choices, advanced sampling methods, and data analysis, with the goal of educating potential polymer simulators about ways to improve the validity, usefulness, and impact of their polymer computational research.

I: Introduction
Polymers are a class of complex fluids that present unique challenges to a computational scientist, as they exhibit interesting and important phenomena over a broad range of length scales starting from monomer (angstroms) to the polymer radius of gyration (nanometers) and time scales ranging from femtoseconds to seconds/minutes or even years (in the case of glasses). Many, but not all, of these computational challenges have been addressed over the years with numerous developments in software and algorithms as well as through significant improvements in computer hardware. It is not surprising that with these advances we have reached a point where molecular simulations of polymers can be done significantly faster than some of the earliest simulations of polymers and on a device as small as a smartphone! Perhaps it is not an exaggeration to say that the power to do polymer simulations is at our fingertips. With these software and hardware advances, a growing number of simulation studies of polymers provide valuable insight into new as well as existing macromolecular materials and through predictions inspire polymer chemists to find synthetic routes to design new materials that show tremendous promise. Yet, we sometimes hear skeptics label simulations as an “easy” and/or “unreal” tool in contrast to (more difficult) theory and (real) experiments. We believe that part of this skepticism stems from a non-negligible number of peer-reviewed articles in reputable journals that present incorrectly/hastily done computational studies and/or predict phenomena that prove to be wrong by follow-up simulations/theory/experiments. Nonetheless, from our perspective, for every one of those incorrect simulation studies, there are many more carefully and correctly done, creative, powerful, and insightful simulations (or in silico experiments) whose predictions have been proven correct by in vitro experiments and/or whose insights have inspired novel experiments.

With this optimistic view of the growing power of polymer modeling and simulations, we feel it is more important now than ever to educate the researchers—new students at undergraduate and graduate levels or experts in other fields who choose to use simulations for the first time to complement their work—of the right/correct ways to design and perform polymer simulations. In our opinion a central challenge with simulations is that incorrect results often do not look unrealistic or incorrect, and therefore care must be taken to validate that the results of a polymer simulation are meaningful. In this Perspective, we highlight some (not all) of the best practices and common pitfalls a researcher should keep in mind as they choose the model and simulation method appropriate for the problem at hand and how they should correctly analyze the simulation trajectories to reach meaningful conclusions. We note that there are other well-written perspective articles, like Frenkel’s “Simulations: the dark side”, which is a must-read for experienced and beginner simulators alike. Another short perspective article from a few editors of Langmuir guides the readers on the qualities of a good theory or simulation paper/study from the perspective of the Langmuir readership. These articles from Frenkel and the Langmuir editors focus broadly on simulations in general, yet many of the guidelines listed in these perspectives are definitely applicable and useful for polymer simulations. In contrast to these articles, we focus this Perspective specifically on modeling and simulations of polymers/macromolecules. We provide examples from our group and other groups from around the world who, like us, work in the areas of polymer nanocomposites, solvent processing of polymers, conducting polymers for organic photovoltaics, and polymers complexing/conjugating with nucleic acids/peptides for biomaterial design—subjects of our past and current research interests. This Perspective is written from a multidisciplinary view because of our philosophy about polymer models and simulations which lies somewhere between that of a chemist (who prefers to maintain atomistic detail) and a physicist (who prefers to design general models for universal behavior).
We emphasize that this Perspective in no way is meant to be a comprehensive review or a substitute for outstanding textbooks on simulations or past review articles on polymer simulations, all of which are excellent resources for novices and experts alike. We realize that because of our specific areas of interest and expertise, we have not discussed many currently trending or classical topics within the polymer field that are important and deserve the same attention as those that we cover here. For example, we have excluded detailed discussions of the models, simulations, and analyses that correctly capture polymer dynamics, nonequilibrium simulation methods that are used to study effects of mechanical shear and/or other external field effects, specific challenges related to polarizable models and simulations of polarizable macromolecular systems, quantum mechanical simulation methods that are relevant to understanding/developing polymer reactions, and so forth.
We have organized the flow of information in this Perspective like a roadmap for performing simulations of polymers, with the sequence of the sections mirroring the workflow of polymer computational research. Figure 1 demonstrates some of the key steps along that workflow, including developing a suitable model (and appropriate model resolution/length scale), building and running the simulation, and analyzing the simulation trajectory to produce useful data. As we present our opinions and views along the various steps in the workflow, we cite and highlight some new and exciting modeling and simulation work in the areas listed above and mention, briefly, some yet-to-be-solved challenges/problems.

Figure 1. Schematic illustrating many of the key steps in performing a polymer simulation. This Perspective will discuss important considerations and best practices when developing appropriate polymer models, building and running polymer simulations, and analyzing and communicating the results.

II. “What Model Should I Use for the Problem at Hand?”
It is vital that those who are thinking of using modeling and simulation as a potential tool for their scientific/technical polymer-related problem first ask the question, “what do I want the modeling and simulation exercise to accomplish?” Only with a clear answer to the above question should one proceed to identify the appropriate model to accompany the simulation approach. The “best model” depends on the system one wishes to study and/or the question(s) that one wishes to answer. Broadly, for classical molecular simulations of polymers most models fall into one of two classes—atomistic or coarse-grained—the latter containing either generic models or models that are parametrized/optimized for specific chemistries.
In general, if the problem at hand asks for an understanding of the local, monomer-level (re)arrangements, fluctuations, or interactions, within a disordered or ordered polymer system, then atomistic models are the appropriate choice. For example, atomistic models can enable study of local monomer-level fluctuations or monomer–monomer contacts/interactions at interfaces of domains within a block copolymer system that is known to phase separate into a specific morphology. Atomistic models have also shown how monomers and segments of polymers arrange near a specific nanoparticle surface within a polymer nanocomposite. Another example where atomistic simulations are useful is in studying gas/small molecule solubility/absorption/diffusion within pores of a polymer membrane, where the specific chemistry and size of the gas/small molecules impact its interactions with and solubilities in preassembled (glassy) polymers. In the case of charged polymer systems, electrostatic interactions and/or entropic driving forces (e.g., translational entropy related to counterions) are critical, and in these cases, atomistic simulations may be useful as one can probe where and how the counterions interact with charged polymers and/or conformations of the charged species within the polymer chains. However, because of their computational expense, atomistic models are usually limited to length scales of 1–100 Å and time scales of 1 fs–100 ns. As a result, these models cannot probe phenomena involving polymer chain-level rearrangements that may occur over larger length and time scales. Despite being restricted to small length and time scales, simulations with atomistic models take a nontrivial amount of time, even on the newest computer hardware with tens to hundreds of CPU cores running in parallel. One may (incorrectly) think that this computational expense is always worthwhile because we obtain an “accurate” atomic-level perspective of these materials. Alas, the accuracy of these atomistic models depends heavily on the force fields (i.e., all the constants in the bonded and nonbonded interaction potentials between all the atom types in the system), the initial configuration one inputs into the simulation, and the chosen simulation protocol. Both the force field and correct initial configurations are not always easily available/optimized for the system of interest, and incorrect use of either or both can lead to incorrect results. If you think atomistic models are likely the best choice for you, you can proceed to section II.A, where we present advantages and limitations of using atomistic models with specific examples.
By contrast, if the goal is to predict structure/morphology for polymeric systems at a broad range of conditions or polymer design parameters, some of which may be without well-tested atomistic force fields/initial structures, the best option is to start with coarse-grained (CG) models. Coarse-graining reduces some of the degrees of freedom in the system by grouping selected atoms in a monomer/a whole monomer/a group of monomers/Kuhn segments together into a single CG “bead”. This approach is possible due to the wide separation of length and time scales inherent to polymer systems—in many synthetic polymers the structure and dynamics at length scales approaching the chain size are largely unaffected by the local monomer structure and/or high-frequency motions of individual atoms. Thus, CG approaches remove some of this atomistic detail to improve computational efficiency and flatten free energy barriers. How “fine” or “coarse” the model (i.e., resolution) needs to be depends on the system/question at hand, as seen in the specific examples described in section II.B. By reducing the degrees of freedom and removing the faster motions in the macromolecule, one can take larger time steps with CG simulations as compared to atomistic simulations. This leads to faster equilibration of large polymer lengths or large system sizes at time scales necessary to probe macromolecular phenomena such as polymer chain relaxation, structural rearrangements associated with disorder-to-order transitions (e.g., in polymer blends or copolymers), or particle dispersion-to-aggregation transitions within polymer matrices (e.g., in polymer nanocomposites). CG models are also useful when one wishes to explore a small system size but for a large range of polymer design parameters (e.g., molecular weights, chemistries, architectures, dispersity, etc.) and/or system conditions (e.g., temperatures, solvents) to identify potentially interesting/useful parameters/conditions for achieving a desired morphology prior to conducting experiments or more expensive atomistic simulations. Simulations with CG models are sometimes unpopular among computational chemists who may view CG models negatively due to lack of atomic detail, while computational physicists view CG models as a powerful technique that enables researchers to obtain universal trends (rather than for one specific chemistry). Technically, CG models can also be developed for specific chemistries to enable simulations that explore structure and thermodynamics for those specific cases. If you think CG models are the best choice for the problem you have, we direct you to section II.B where we describe types of CG models, methods for CG model development, and examples for various CG model resolutions.
II.A. Atomistic Models
Because atomistic models probe length scales that are significantly shorter than the typical contour lengths of polymer chains composed of >100 repeat units, long time scale relaxation of a single chain and/or rearrangement of many such polymer chains during an atomistic simulation is out of the question. Therefore, it is best to use atomistic models to study systems where the chain configurations at the condition(s) of interest (i.e., temperature, pressure, and concentration) are known a priori through either other calculations or experiments. Simulations with atomistic models are used for characterizing (a) specific atomic-level interactions in a known morphology, (b) intermolecular interactions within the chain and between the chain and solvent/additive/ion that stabilize a secondary structure or crystalline state of the polymer, (c) interactions between two polymers that bind/complex together, and so forth. For example, in our group, we have used atomistic (or all-atom) models to describe interactions between two charged polymers where one or both have a known secondary structure, to understand how poly(ethylene glycol) conjugated to polypeptides impacts the polypeptide conformations, to understand how solvent (water) interacts with an elastin-like peptide chain to drive its phase transition, and so forth. In all of these cases, we either did not know a priori the key molecular driving forces for the systems at hand, and hence did not have the insight to develop coarser models (like those in section II.B), or had scientific objectives that necessitated this level of chemical detail.
In many cases where atomistic details are needed, one may not need to explicitly represent the hydrogen atoms, which have high-frequency motions that enforce small time steps in the numerical integration and, as a result, limit computational efficiency. In that case, one may opt to use united atom models, in which the hydrogen atoms are not represented explicitly and are combined (in terms of mass and position) with the heavier atom they are attached to. Even though the united atom models can be advantageous in terms of improved computational speeds, it is important to note that modifications must be made to a united atom representation to capture aspects of structure and thermodynamics that might be driven by hydrogen bonding. Furthermore, the lack of hydrogens can artificially accelerate atomistic dynamics, and united atom results may also deviate from true behavior due to differences in partial charge representation.
If an atomistic model is the appropriate choice for the problem at hand, usually the first step is to find the right force field from the many available choices relevant to polymers. Each force field has a unique set of parameters (and sometimes varying functional forms) for the various terms in eqs 1 and 2, where the total potential energy of the system is some function of the atomic coordinates. 

In the preceding equations, Ri are the coordinates of all the atoms in the system, kjl and lj0 are the bond length constant and equilibrium length of bond j, kjθ and θj0 are the bond angle constant and equilibrium angle of bond angle j, kjϕ is the dihedral angle constant of dihedral angle j, qi is the charge on atom i, rij is the distance between atoms i and j, εij is the strength of dispersion interactions between atoms i and j, and σij is the contact distance between atoms i and j. We note that eq 2 is an example of one possible way to represent the terms in eq 1; there are many variations and options for the specific functional form of each term, the details of which are beyond the scope of this paper.
Computational chemists optimize and parametrize force fields for organic molecules in general (compatibilized for use with simulation packages, e.g., refs) as well as for specific macromolecules that exhibit secondary structures (e.g., proteins and nucleic acids) or crystalline structure (e.g., polyethylene and poly(3-hexylthiophene)). These force fields have been tested to ensure they reproduce relevant experimentally observed polymer physical properties (e.g., density, cohesive energy, melting temperature, glass transition temperature, etc.) at specific conditions (e.g., temperature, pressure, concentration, and composition). If one is unable to find a force field that was developed exactly for the polymer/system of interest, it is best practice to select a force field that was optimized for chemistries and/or conditions as close as possible to the system of interest. It is also important to educate oneself and the readers of their work about the limitations of the chosen force field’s transferability, i.e., the ability of the force field to accurately simulate conditions other than those for which it was designed/optimized. Atomistic force fields are parametrized to match some set of reference data at a specific set of conditions (e.g., temperatures/pressures/concentrations), and the constants and terms in eqs 1 and 2 may not necessarily hold as the system departs from those reference states. For example, it is not uncommon for researchers to use polymer force fields optimized in the presence of a solvent to simulate a polymer in a vacuum (e.g., for a quick screening procedure without excessive computational intensity involved in explicit modeling of the solvent). This strategy is not necessarily inappropriate if such an extension is done in cases where the solvent plays a passive role in the system behavior, but the user should be careful to not overemphasize results that show minor energetic or structural differences and instead focus on larger qualitative trends. Similarly, force fields that are developed for use in a specific range of temperature and pressure should not be used at conditions far from those ranges without validation or corrections. For example, upon selection of the appropriate force field for water molecules, it is important to select a force field that can reproduce experimental observations at conditions relevant to the problem at hand (e.g., ice–liquid water phase behavior at low temperature and/or at high pressure versus liquid water behavior near solvated biomolecules at room temperature and atmospheric pressure).
In the case that the “best” force field for the polymer chemistry and conditions of interest is not immediately clear, it may also be worth testing multiple relevant force fields to ensure the key results/insights are not sensitive to the choice of force field. For example, in a recent work from our group we tested the choice of force fields on structural signatures underlying the inverse temperature or lower critical solution temperature (LCST)-like transition in aqueous solutions of short elastin-like polypeptides (ELPs), which are considered to be intrinsically disordered proteins. In this case, we found that the choice of force field did not alter our conclusions about the lack of signatures for LCST in the time evolution of the squared radius of gyration (Rg2) of short ELP strands. However, in other cases authors have reported a dependence of the observations (e.g., propensity for one secondary structure over another, structural arrangement of water molecules) on the choice of force field.
The successful implementation/execution of simulations using these atomistic models will also depend on what you start with (i.e., the initial configuration) and the choice of the simulation algorithm (see section III). Databases of previously obtained crystallographic structures for the polymer of interest are an ideal starting point when looking for a relevant atomistic initial configuration. But such structures are most often only available for biomacromolecules with known secondary or tertiary structures (e.g., proteins, DNA, RNA, etc.) or for polymers with crystalline domains. In the case of amorphous polymers or polymers whose structure is not known, one can rely on a molecular builder software and energy minimization software  with the selected force field that together create the energy-minimized initial chain configuration for a given polymer chemistry at a specific condition. In some cases, techniques for back- mapping or reverse-mapping from final configurations of simulations of less detailed (i.e., CG) models can also be used to create realistic atomistic initial configurations. 
If you have identified a good force field and obtained a correct initial configuration, you can proceed to section III where we describe how to simulate the chosen system with the atomistic resolution.
II.B. Coarse-Grained (CG) Models
CG models of polymer chains are composed of “beads” with each (usually spherical) bead representing some portion of the polymer chains. The length scale of the mapping between full atomistic detail and a CG bead can vary wildly, and this mapping is one of the key choices made in choosing/developing an appropriate CG model. CG beads can represent a specific group of atoms within a monomer (which we term “intermediate resolution”), a whole monomer (we call this “CG monomer”), a group of monomers on the length scale of a Kuhn segment, a group of Kuhn segments, or even the whole chain (termed “mesoscale” CG models). The former (more “fine-grained”) models are usually developed/parametrized to represent specific polymer chemistries, while the latter (more “coarse-grained”) are usually generic and provide universal properties (e.g., scaling laws relating Rg to molecular weight) for a large family of polymers. In some cases, researchers have developed phenomenological CG models that look like intermediate-resolution CG models of specific chemistries but mimic phenomena at length and time scales much larger than the intermediate-resolution CG models. Once a suitable spatial mapping has been decided, interactions between the CG beads (e.g., monomer–monomer excluded volume, chain stiffness, electrostatic interactions, etc.) must also be defined to reproduce key structural or thermodynamic properties of interest—sometimes this choice can be an iterative process involving many candidate CG resolutions and sets of CG interactions. Instead of reviewing all of the studies that used each of the above model types, we give a few examples of the types of problems, especially from our own experience, where each of the above types of models is most appropriate.
In Figure 2, we show an example of an intermediate-resolution model that we developed to understand effects of polymer architecture in polycation–polyanion complexation, in the context of designing polycations for DNA delivery. In this example, various parts of the monomer unit are modeled with different CG beads leading to a multiple-CG-bead per monomer model (Figure 2a,b). The sizes of CG beads and the bonded and the nonbonded potentials between all CG beads were chosen to reproduce the relative stiffness of various parts of the monomer and the placement of charges as seen in atomistic simulations and to ensure that electrostatic interactions are the dominant driving force in this problem. These were all details necessary to reproduce the correct structural arrangements during an electrostatically driven complexation. There are other ways to parametrize such intermediate resolution CG models, for example, by directly mapping from atomistic simulations using iterative Boltzmann inversion for structure or force matching or relative entropy matching. In comparison to analogous (yet smaller length scale and time scale) simulations with all-atom or united-atom models, this intermediate-resolution CG model affords a higher computational speed, not only because of the grouped representation of monomer atoms but also because we treat solvent molecules implicitly (i.e., solvent effects are captured through effective polymer bead–polymer bead pairwise interactions). However, this implicit solvent strategy is not a good practice if the solvent plays an active role in the complexation or if hydrodynamics is an important factor in the assembly/chain conformations. In section II.C we discuss a few examples where an explicit representation of polymer–solvent interactions may be important.

Figure 2. The specific chemistry (a) that was modeled using the intermediate-resolution CG model (b). These simulations were done to design comb polymers that complex with DNA to produce polyplexes (c) with desired features that experimentally lead to higher transfection efficiency.

In the studies presented in Figure 2, the polyanion, DNA, was represented using a generic CG (one bead per repeat unit type) model because the details of the hybridized DNA duplex structure and/or DNA duplex melting were not important to the scientific objectives of the work. There have been many other CG models developed for DNA). In particular, if the problem at hand requires a faithful reproduction of the duplex structure (e.g., minor and major groove, rise, and twist), one should use intermediate-resolution models for DNA, like the 3-site per nucleotide model for DNA (Figure 3b). Alternatively, if one is interested primarily in the melting transition of DNA duplex for a given sequence, concentration, or DNA length, but not the duplex structure, one could opt for less-detailed models like our recent work, where we represent the nucleotide with two CG beads per nucleotide (Figure 3c): one “parent bead” representing the base and the backbone chemistry and another “sticky bead” for capturing the directionality of the hydrogen bonds. This model captures interstrand base–base hydrogen bonding and intrastrand base–base stacking which dictate the thermal stability of the DNA duplex. The neighboring-base stacking effects are captured with bond angle and dihedral potentials along the “parent-bead” backbone (large beads in Figure 3c), and the directional and specific base-pairing interactions are captured with a combination of bonded potentials and attractive nonbonded potentials between the “sticky beads” (small yellow beads in Figure 3c). Besides DNA, there are other polymer chemistries (synthetic or biological) that have specific and directional molecular interactions, and the inability to capture small-scale specific and directional interactions (e.g., hydrogen bonds) in these systems while probing macromolecular length and time scales represents a key limitation of most CG models. To address this limitation and to create a general CG model for such polymer chemistries, recently we have modified the two-bead CG model described for the DNA double helix above to enable the representation of other macromolecular systems where anisotropic, directional, and specific interactions (e.g., hydrogen-bonding interaction) govern the structure and thermodynamics in those systems (e.g., collagen mimicking peptides that form triple helices due to hydrogen bonding). In contrast, other CG models obtained via systematic atomistic to CG mapping approaches represent specific and directional interactions in biomacromolecules usually via effective isotropic interactions.

Figure 3. (a) All-atom (left) and coarse-grained (right) representations of a section of double-stranded DNA; the CG representation uses the 3-site per nucleotide CG model. (b) Superimposed image of the 3-site per nucleotide CG model (spheres) over the all-atom representation (sticks). (c) A representative illustration of the spatial mapping of beads to nucleotides and a schematic illustration of oligonucleotide duplex strands in our recently developed hydrogen-bonding CG model.

Another class of polymers that can be represented with such intermediate-resolution models are conjugated polymers (e.g., poly(alkylthiophenes)) that have a conjugated backbone (e.g., thiophene rings) and alkyl side chains. These structural motifs play important roles, as the π-stacking of the conjugated backbones induces crystallization, while the alkyl side chains improve solubility in common organic solvents. Figure 4 (middle panel) shows an example of an intermediate-resolution model for poly(3-hexylthiophene) (P3HT) and poly(2,5-bis(3-alkylthiophen-2-yl)thieno[3,2-b]thiophene) (PBTTT) that we developed to capture both the backbone–backbone stacking-induced orientational order and the packing of fullerene derivatives between side chains in polymers with wider spacing between the side chains (e.g., PBTTT). There are atomistic simulations of these conjugated polymer chains, but an all-atom simulation of these systems (like the image in Figure 4, left panel) at meltlike conditions will never achieve a crystalline state from a disordered amorphous melt in any reasonable time scale, without aid from other theoretical methods (see section IV.B) or advanced sampling methods (see section III.A). Therefore, if one is interested in using simulations to predict the ordered meltlike state in new polymer architectures or chemistries without prior experimental knowledge of structure, the atomistic approach will not be a viable option. The better choice for such a quest may be intermediate-resolution CG models or even coarser CG models (e.g., Figure 4, right panel) that sacrifice the details distinguishing the side chains from the thiophene rings and using a one bead per monomer representation focus on larger scale domain formations rather than intermolecular packing. We note, however, that the resolution of the CG model impacts the ability of the simulation to correctly reproduce the experimentally measured physical properties (e.g., density and tensile modulus) of the specific polymer chemistry.

Figure 4. Three examples of models of conjugated polymer–fullerene blends at varying model resolution. The left image is an atomistic model, the center image is an intermediate-resolution model with multiple beads per monomer, and the right image is a one bead per monomer model

The examples of intermediate-resolution CG models described above were developed for specific chemistries or specific phenomena (i.e., hydrogen bonding), but many researchers prefer generic intermediate-resolution CG models that can be applied to a larger family of polymer chemistries. In this regard, we mention the general MARTINI models, where 3–5 heavy atoms are grouped together to form a CG bead and each CG bead is assigned a classification (polar, nonpolar, apolar, donor, acceptor, etc.) based on the chemistry/connectivity of the atoms it encompasses. Thus, one can build a CG representation of a macromolecule directly from its chemical structure, while maintaining chemical detail over the length scale of 3–5 atoms. However, in most cases one may need to refine or optimize the base MARTINI model’s bonded and nonbonded interaction potentials to more faithfully model a specific polymer chemistry, sequence, and architecture. For example, one may choose to alter the strength of the pairwise Lennard-Jones interactions and/or create new dihedral interactions between the standard MARTINI beads to better agree with experimental observations.

In some studies, one may model a whole monomer or a group of monomers with a single CG bead. One example where this type of model may be a good choice is that of phase behavior of polymer blends where the monomers from both polymers have the same/similar size, but the Kuhn segment or persistence lengths of both polymers are not necessarily the same (e.g., chains of different stiffness or small molecular weight linear and cyclic homopolymers). This level of resolution also lends itself to the study of the effects of changing solvent quality on chain conformations in polymer solutions when the monomer and solvent are similar in size (e.g., polystyrene and toluene) and the solvent has to be explicitly represented. Coarse-graining solvents, especially water, has been a topic of significant interest in the physical chemistry and statistical thermodynamics community (albeit less so in the polymer computations community). The tetrahedral ordering of the water molecules due to intermolecular hydrogen bonds has inspired some CG models (e.g., MARTINI and BMW) where four water molecules are grouped into one CG bead. In such cases where multiple solvent molecules are represented with one “solvent” CG bead, one may wish to model multiple monomers along the polymer backbone as one “polymer” CG bead to maintain similarity in size between “solvent” and “polymer” CG beads. The multiple monomers/Kuhn segment per CG bead approach is advantageous, especially if one is studying large molecular weights (>100 monomers) or systems with large length scale density or concentration fluctuations (e.g., comparing to low wave vector information from scattering measurements). Such CG bead models (also commonly known as Kremer–Grest models) are widely used, with the bonded interaction commonly represented with a harmonic bead–spring potential or the finite extensible nonlinear elastic (FENE) potential. These models have often been used with implicit solvent approaches to speed up simulations; in these implicit solvent simulations appropriate solvent-induced nonbonded interactions between the CG beads along with some stochastic random forces capture the solvent effects implicitly. In polymer melts, the question of solvent representation is eliminated, and thus, the Kuhn segment per CG bead representation is a reasonable choice. Figure 5 is an example of the use of such a model in a large polymer nanocomposite system composed of matrix polymer chains and polymer grafted nanoparticles at a meltlike packing fraction. In these systems, aggregation/dispersion of the grafted particles is identified through the low wave vector behavior of the particle–particle structure factor (a description of this calculation is in section III.B). These Kuhn segment per CG bead models are also good for calculating a general phase diagram of order–disorder transition and the various ordered morphologies for melts of copolymers with varying sequences (e.g., diblock, triblock, random, tapered, etc.) or architecture (e.g., linear, star, comb, graft, bottlebrush, cyclic, etc.). For copolymers and other multicomponent systems, the cross-interactions between unlike species requires special attention—the use of empirical combining rules is known to have significant effects on phase behavior, and careful methods to set the pairwise interactions for unlike species have been proposed.

Figure 5. Simulation snapshots of polymer grafted nanoparticles (silver beads for particles and blue beads for grafted polymers) in an explicit homopolymer matrix (green beads). The left image shows all components of the simulation box, the center image shows representative chain configurations for the grafted and matrix chains, and the right image is the simulation box showing only the polymer grafted nanoparticles with all matrix chains hidden.

The Kuhn segment per CG bead model can also be extended to a coarser limit in the form of lattice polymer models. In these lattice models, used primarily with Monte Carlo (MC) simulations, each CG bead in a polymer is placed on a lattice site with the bonded neighboring CG bead placed on one of the nearest/next-nearest-neighbor sites on that lattice. The cubic 3D lattice has been used in most studies, while face-centered (3D) and square (2D) lattices have been used less frequently. The choice of the lattice and type of lattice model dictates the distribution of bond lengths and bond angles. For example, in the simplest version of the cubic lattice model, all bond lengths are the same (equal to the lattice spacing) and bond angles are limited to two values: 90° and 180°. In the bond fluctuation model, the CG beads along a polymer chain are connected via one of 108 possible bond vectors, making this model more realistic than the simple cubic lattice model. The interaction potentials between the nonbonded polymer beads are modeled through hard sphere interaction potential or single/multiple square well potentials. Compared to their off-lattice or “continuum” counterparts, these lattice models can sample many more configurations in much less (computer) time due to the coarser representation of the chain, a fewer (discretized) number of bond vectors, a higher fraction of acceptances of MC moves through easy identification of overlaps, and so forth. These advantages make these models useful when one wants to study a problem at large length and time scales (e.g., polymer melts) where atomistic or off-lattice CG models are not feasible. There are some disadvantages of lattice polymer models: the vacancies (i.e., unoccupied sites) have the same size as the polymer CG bead, these models essentially capture only the configurational part of the partition function, and one cannot calculate the forces and momentum making them unsuitable for MD simulations. Also, as pressure cannot be calculated easily with these models without complex algorithms, lattice approaches are generally not preferred for performing constant pressure simulations.

So far, most (not all) polymer CG models that we have described utilize nonbonded interaction potentials like the Lennard-Jones potential or square well potential, which have a strongly repulsive part (to represent the bead excluded volume) and an attractive part. Rather than using the strong excluded volume of the LJ monomer, one may wish to simulate polymers as a chain of soft CG spheres, with each CG sphere representing many monomers in a real polymer (or the whole polymer itself), and these CG spheres interact through purely repulsive soft potentials. Such models are often used in dissipative particle dynamics (DPD) simulations. Even though all pairwise interactions are repulsive, effective attraction can be captured by the choice of relative repulsions such that the two beads that repel each other less than they do other beads are effectively attracted to each other. These types of models fall under the class of “mesoscale” models, which permit large timesteps and, hence, are the best choice to capture phenomena that occur on large time scales (e.g., assembly of large molecular weight polymers into ordered morphologies from disordered states). In the context of this type of mesoscale model, we direct the reader to a recent perspective article on DPD simulations that describes the many advances made on the original DPD method, including many-body DPD, incorporation of electrostatic interactions in polymers, and so forth. As noted in many DPD studies, these soft CG models and DPD simulations have some drawbacks, such as (unphysically) high compressibility, loss of explicit effects of torsional/topological constraints between chains, and a lack of chemical detail, some/all of which can be key to the underlying complex phenomena.

In the context of mesoscale models, it is important to mention the field theoretic computer simulation (FTS) approach developed by Fredrickson and co-workers for polymers. In essence, in FTS calculations one integrates out the explicit bead/particle coordinates in the partition function and replaces them with “functional integrals over one or more fluctuating chemical potential fields” within the simulation domain. We direct the reader to the original articles by Fredrickson and co-workers and recent work by other groups for the details and applications of these FTS methods.
As we mention in brief above, significant effort has gone toward developing systematic coarse-graining approaches to automate the CG model-building process. These approaches can improve the accuracy and transferability of CG models, particularly for biomolecules, conjugated polymers, semicrystalline polymers, and other systems, where faithful representations of the local chemistry are necessary to correctly reproduce long length scale structure and dynamics. A complete discussion of these approaches is outside of the scope of this work; we refer the reader to the many excellent book chapters, reviews, and journal special issues on this subject.

II.C. Hybrid/Mixed Resolution. 
In biologically relevant polymer systems, CG models with implicit representation of the solvent may not be appropriate to interrogate important questions, as the solvent (especially water) can significantly affect the structure of bioinspired/biomimetic/biological macromolecules through both direct hydrogen-bonding and indirect hydrophobic interactions. For example, in biomaterials with poly(ethylene glycol) (PEG), the affinity of PEG for water molecules creates a hydrated layer around the PEG chain, which in turn impacts the interaction of PEG with other biomacromolecules. Another reason to explicitly represent water/solvent molecules is if one wants to include hydrodynamics to correctly capture the dynamic behavior of the macromolecules. But, representing water or other solvent molecules explicitly and with atomistic detail leads to a large increase in the computational demands, as the majority of the simulation time is spent on calculations involving the many solvent molecules/atoms instead of the polymer(s). This trade-off has motivated the development of hybrid/mixed resolution models (e.g., models with both atomistic and coarse-grained representations) “where molecules can either stay fixed in separate resolution domains or freely move between them”. For example, a hybrid resolution model would be one where the polymer and the solvent molecules near the polymer are modeled atomistically, while the solvent molecules far from the polymer chain are modeled in a CG manner. As the solvent molecules diffuse from/to the atomistic zone to/from the CG zone, their resolution changes to that of the destination. These simulations have the challenge of having to create a hybrid zone between the CG and atomistic resolutions so as to avoid density gradients between the all atom (AA) and CG zones. This is the main idea behind the adaptive resolution scheme (AdResS) methodology, which we have used in our research to understand how hydration shells of PEG chains vary with changing polymer lengths and architectures (Figure 6).

Figure 6. The left image is a schematic showing a solvated macromolecule surrounded by water molecules represented atomistically near the macromolecule (denoted as AA zone) and represented in a CG mode far from the macromolecule with a hybrid (HY) zone sandwiching the AA and CG zones. The right image illustrates this transition as a function of radial distance from the macromolecule.

III. “How Can I Simulate the System at Hand and Get Meaningful Data?”
III.A. Performing Simulations
After a suitable model has been chosen, the next step is to select the simulation method; as with model development, the appropriate simulation technique depends heavily on the system and scientific questions of interest. In section II we briefly mentioned appropriate simulation methods that are suitable for each type of model, but in general the two workhorse molecular simulation methods for polymers are Monte Carlo (MC) and molecular dynamics (MD). Detailed descriptions of these techniques, their benefits and drawbacks, and important extensions/modifications of the two approaches have been extensively reviewed elsewhere as well as their specific applications to polymer systems, so we direct the readers to those articles. Briefly, in MC simulations trial configurations are created using stochastic displacements of atoms/beads, and these trial moves are accepted or rejected by some criteria (e.g., Metropolis algorithm) that is usually a function of the total potential energy of the new and old configurations. Then, thermodynamic and structural quantities are computed by performing averages over the accepted configurations. Care must be taken to use/develop appropriate MC moves that have a reasonable probability of being accepted and enable proper sampling of the relevant configurations for that state. For instance, local moves (e.g., stochastic displacement of individual monomers within a chain) can be used to target dynamical information but are slow to relax chain configurations, while nonlocal moves (e.g., pivot, bridge, configurational bias regrowth, etc.) provide drastic changes in chain conformation at the cost of nonphysical jumps between successive configurations. In contrast, MD algorithms numerically integrate Newton’s equations of motion to calculate a deterministic trajectory of the component atoms/beads as a function of time; “snapshots” from this trajectory are used to compute configurational averages. We have found that MD simulations are advantageous because they can be simpler to implement than MC in some cases when complicated polymer regrowth moves or configurational bias methods are needed in MC, and MD is also more easily parallelizable than MC, which makes MD a more computationally efficient choice for large systems. However, standard MD simulations can become trapped in kinetically limited states, and because of the slow relaxation of polymer chain conformations, MD can require prohibitively long simulation times to achieve equilibrium configurations in dense or atomistically detailed systems. In such cases, MC simulations or MC moves interspersed with MD simulations are often used as a way to relax the system out of its trapped morphology/structure. Regardless of the chosen method, we can separate a standard MD or MC simulation into three loosely defined stages: (1) system creation and initialization, (2) equilibration, and (3) sampling. An aspiring simulator should design system creation, equilibration, and sampling protocols that avoid bias in the obtained results, and there are several key considerations to keep in mind. As with the model development section, we do not attempt to provide a comprehensive review of all possible simulation methods and sampling techniques; rather, we present a step-by-step illustration of some important steps in performing a polymer simulation, mostly in the context of recent (MD simulation focused) work from our group or by highlighting other important/creative efforts in the community.
Choosing an Ensemble
Once the choice of MC versus MD has been made, the user must select the type of thermodynamic ensemble to sample in the simulation. For instance, MD in its most basic form samples the microcanonical or NVE ensemble, where the number of particles, volume, and total energy of the simulation box are maintained constant. However, by coupling the motion of the particles to a heat bath through a thermostat, one can maintain the kinetic energy of the system at a specific temperature, sampling the canonical (NVT) ensemble. Similarly, if the box size is allowed to fluctuate to maintain constant pressure, the NPT ensemble can be accessed. NPT simulations are generally the most directly comparable to a given experimental condition (as constant pressure and temperature are accessible in the experimental lab); however, a fluctuating box size can introduce some complexities in analysis or lead to finite box size effects (see discussion of this topic below). Therefore, it is necessary to monitor equilibrium box sizes when using the NPT ensemble. There are also other less commonly used (and sometimes less rigorously defined) ensembles that can be relevant/useful in specific cases, such as constant normal pressure, constant surface tension, and constant temperature (NPγT) ensemble or constant interfacial area simulations for studies of interfacial properties in membranes/fluid–fluid interfaces. If one is interested in studying phase behavior of polymer blends and solutions, Gibbs ensemble or grand canonical ensemble MC simulations allow the user to probe phase coexistence in a computationally efficient manner by leveraging bead insertion/deletion moves to perform simulations with a fluctuating system size. Both of these techniques have been widely applied to study phase behavior in polymeric systems due to the fact that they enable the study of phase coexistence without expending the resources to model the interface between phases. Recently, we proposed an alternative Gibbs ensemble MD method (for small molecules but in principle can be extended to polymers) that makes use of the relative simplicity and parallelizability of MD for macromolecules while retaining the advantages of the Gibbs ensemble approach.
Choosing System Size
After the method and ensemble have been selected, the next decision is the size of the system (i.e., number of atoms or CG beads and/or initial simulation box size). As with any computational technique, avoiding system size effects (bias or artifacts arising from the mathematical treatment of the boundaries of the simulation domain) is critical. For instance, accurate calculations of thermodynamic descriptors such as the Flory–Huggins χ parameter or polymer–polymer second virial coefficients (B2) require access to long length scales, thus necessitating large simulation boxes with hundreds of thousands of atoms/beads. Similarly, efforts to study phase separation necessitate box sizes much larger than the length scale of the concentration fluctuations to avoid unphysical results. In contrast, if one is interested in dilute-solution properties like chain configurations and scaling laws, often single-chain (or few-chain) simulations are appropriate. If one chooses smaller system sizes to achieve simulation results more quickly, one must account for the inaccuracies that enter their calculations due to “finite size effects”. Finite size effects in simulations with periodic boundary conditions emerge from self-interactions across periodic boundaries and/or from the limited size of the model system and/or box size. Choosing smaller system sizes can lead to large pressure and stress fluctuations which, for example, can prevent the accurate calculation of the mechanical properties of a polymer system. Some researchers have identified that depending on the system, avoiding deleterious pressure fluctuations requires system sizes larger than 20000 atoms/beads, and sometimes systems on the order of 100000 atoms/beads or larger are preferable. Furthermore, finite size effects can also influence polymer dynamics, electrostatic interactions, and free energy calculations. From the work of ref (314), a good rule of thumb identified for polymer melts is that the ratio of box length L to Rg should be L/Rg ≥ 5 to limit the influence of self-interactions on chain dynamics. We note that the finite size effects are not limited to the issues listed above; in fact, the various types of possible finite size effects in simulations have warranted a whole workshop on this topic recently.
Selecting an appropriate system size to maximize one’s computational resources while providing trustworthy results is a nontrivial task, and there is no substitute for experience. The most straightforward route to establish that the system size is not biasing the results is to perform an identical simulation with varying system sizes; as the system size increases, beyond a certain size (the optimal system size) the results should be insensitive (within error) to the system size.
Building the Initial Configuration
If the system to be studied is isotropic, a naïve initial placement of the components of the system may be sufficient (e.g., randomly dispersed throughout the simulation box or with molecules placed on some predefined lattice). However, at high (meltlike) densities, diverging interbead forces resulting from highly overlapping atoms/beads may cause numerical instabilities or at least result in unphysical perturbations in chain configurations which can take prohibitively long equilibration times to remove due to slow chain relaxation. Thus, careful “construction” of the simulation’s initial configuration is critical to limit unnecessary equilibration time and/or prevent bias. For example, Auhl et al. proposed a useful technique to prepare long-chain CG melts with near-to-equilibrium chain configurations. They initially generated an ensemble of random-walk chains in the simulation box with no regard for intermolecular overlaps, then performed a “prepacking” MC algorithm that rotated and translated the chains to minimize density fluctuations while keeping intrachain structure fixed. Then they slowly introduced excluded volume between CG beads during an MD relaxation stage using a cosine-based “soft” potential with a progressively increasing repulsion strength. This “slow push-off” procedure produced meltlike initial configurations with significantly reduced perturbations in local chain conformations, as shown in Figure 7. Related recent work proposed additional modifications to the “push-off” procedure that further improved efficiency. In both efforts, an important metric used to track the chain configuration is the mean-squared internal distances (⟨R2(n)⟩/n), which is a measure of the distance between atoms or beads separated by n bonds along the chain. This allows for the detailed examination of chain dimensions over all length scales and can show perturbations in internal chain dimensions that would not be observable with chain-averaged properties such as radius of gyration (Rg) or end-to-end distance (Ree).

Figure 7. Mean-squared internal distances for initial polymer melt configurations of length 25, 50, 350, and 7000 beads (dotted lines and +, ×, ∗, and □ symbols, respectively) and an equilibrated configuration (thick solid line) after (a) a “fast push-off” procedure and (b) a “slow push-off” procedure. The “slow push-off” significantly decreased initial perturbations in chain dimensions as compared to the equilibrium structure.

An alternative strategy involves starting with CG soft spheres that represent many beads along a chain, then using back-mapping procedures to progressively increase the detail of a simulation, and maintaining correct chain configurations as the length scale of the coarse-graining decreases through local equilibration steps. Similarly, if one were to create initial configurations of polymer chains with intermediate-resolution CG models that represent specific chemistries, one could start from energy-minimized atomistic structures to guide placement of the various bonded CG beads in a monomer and chain. Using some or all of these ideas during system preparation can significantly decrease the amount of required equilibration time.
If nonisotropic or phase-separated systems are of interest (e.g., fluid–fluid interfaces, self-assembled bilayers, micelles, vesicles, or other microphase separated structures) and these structures are known, then it may be necessary to construct the initial condition of the simulation in a configuration that is close to the desired structure, to avoid waiting a potentially prohibitively long time for the desired structure to spontaneously assemble in the simulation box. Many common simulation packages are equipped with their own system-building tools (e.g., LAMMPS) or can interface with other open-source packages such as PACKMOL and Moltemplate that allow control over the spatial and orientational configuration of inserted molecules. For example, in determining the domain spacing and interfacial width of lamellar-forming microphase-separated block polymers, one could start with a domain spacing consistent with scaling laws and generate initial A-rich and B-rich regions. Then, using NPT simulations with the box size allowed to float in the direction perpendicular to the domain interfaces, the correct domain spacing can be achieved. Another recent example is our work studying diblock polymer-grafted nanoparticles as compatibilizers for immiscible polymer blends where we initialized an A–B blend in a phase-separated state with polymer-grafted nanoparticles located at the A–B interface. Then, we performed NPT simulations with a constant A–B interfacial area to study the nanoparticles’ impact on the A–B interfacial tension and the structure of the interface. In both of these examples of microphase- and macrophase-separated systems, achieving a well-ordered and aligned system would not be easily accessible using standard particle-based simulations from a random initial state, necessitating careful initialization to examine interfacial characteristics. Similarly, if one wishes to study the melting or assembly/disassembly transitions of macromolecular systems, it is often advantageous to start from the assembled state to avoid waiting for spontaneous assembly (e.g., studying the melting transition of oligonucleotides and other associating biopolymers). However, care must be taken when artificially building assembled states to ensure that the initial configuration is realistic and does not trap the system in unphysical structures. To guide the choice of initial structures, one could use information (e.g., domain sizes in block copolymers and crystal structures of biomacromolecules that exhibit secondary structure) available from theoretical (e.g., self-consistent field theory) results and/or experimental techniques (e.g., NMR, X-ray crystallography, and neutron reflectivity).
Equilibration
After construction of the simulation domain and constituent molecules, equilibration must be performed to remove structural artifacts of the system initialization. We note that simulations do not provide certainty that a “true” equilibrium state has been reached, as there is always the possibility for the system to be trapped away from the true global free energy minimum. For most polymer systems, relaxation in chain configuration is slow relative to easy-to-calculate quantities like total potential energy or pressure, so chain-level metrics are often required to assess whether the system has equilibrated. Internal distances (such as the mean-squared internal distance discussed above) are useful and sensitive examples of such metrics. Another common heuristic is to track the mean-squared displacement of the center-of-mass of the polymer chains; if on average chains have diffused some multiple of their Rg over the equilibration period, the system is often treated as sufficiently relaxed. Once the system reaches a state where desired output quantities are fluctuating around some mean value, then one can proceed to sampling.
If the above basic “brute-force” equilibration at the desired thermodynamic state is not sufficient to achieve an equilibrated system, then by manipulating the system temperature or pressure (or other quantities, such as the strength of nonbonded interactions) in a so-called “annealing” procedure, one can help the system traverse the free energy landscape more efficiently. Annealing protocols are especially important when the system can undergo significant changes in mobility or relaxation time, for example, when nearing the glass transition, order–order or order–disorder transitions, and/or phase separation. One can systematically “quench” the system from a highly mobile state to the less-mobile state to appropriately sample configurations that may otherwise be inaccessible in reasonable time scales if only basic equilibration is used. Through appropriately chosen annealing schedules one can also mimic polymer processing techniques, such as thermal annealing in polymers, the solvent evaporation in polymer films, the amphiphilic assembly of polymers induced by gradual changes in solvent quality, or polymeric nanoparticle formation through rapid quenches in solvent quality (e.g., flash nanoprecipitation process). The systems can also be sampled during the progress of the annealing schedule to locate important transition points (e.g., glass transition or order–disorder transition). As with all other important methodological choices, it is important to test multiple annealing schedules to assess their impact on the obtained results.
Sampling
When sampling the equilibrated state, it essential to ensure that there is sufficient separation (either in time for MD or in configuration space for MC) between snapshots that are used for data collection—in other words, one must ensure that collected configurations for data analysis are not correlated. This is especially important for polymer simulations as their relaxation times can be many orders of magnitude longer than simple fluids. An easy method to ensure uncorrelated data points is to calculate the time autocorrelation function of a property of interest (e.g., average chain Rg). (6) The number of timesteps before the value of the autocorrelation function drops to zero is related to the relaxation time in the system; if the separation between frames saved for data analysis is greater than this number of timesteps, then the snapshots are likely uncorrelated. Finally, one needs to run the sampling step for long enough that desired quantities can be reliably calculated, which may depend heavily on the system and questions of interest. Systems with high energy barriers to rearrangement (e.g., assembled block copolymer micelles, glassy polymers, etc.) may not undergo significant structural changes over the course of a single simulation. Thus, merely performing a single (or small set of) simulation(s) for a longer amount of time may not improve the sampling statistics. Instead, one may wish to carry out many independent replicates of the simulation protocol and average over independent trials rather than independent configurations obtained from the same trial.
The above discussion has mostly focused on standard MC and MD simulations; however, there is a vast array of literature proposing advanced methods that either alter or augment these standard techniques to improve the efficiency and/or precision of a polymer simulation. As noted above, much more comprehensive review efforts/books have been written on this topic, so we highlight a few advanced sampling techniques that we have found useful in our recent work and/or are of significant current scientific interest in the context of polymer systems.
Advanced Sampling Methods
Large-scale particle-based simulations (regardless of model type) are limited in the time and length scales that can be probed, complicating the study of “rare events” such as macromolecular phase or conformational transitions. In principle, a given system’s free energy landscape should be explorable using a single standard simulation, but this may never occur in a reasonable research time scale. Numerous enhanced sampling strategies, such as parallel tempering/replica exchange, umbrella sampling, Wang–Landau sampling, adaptive force biasing methods, metadynamics, local elevation method, and expanded ensemble methods, have been employed to explore free energy landscapes with multiple energy barriers between minima more efficiently than standard (unbiased) simulations. Parallel tempering/replica exchange and umbrella sampling techniques leverage multiple “replicates” of a particular system that are constrained at different state points (e.g., at different temperatures or a macromolecule constrained at different distances from a surface or interface) to allow sampling of difficult-to-access configurations. For example, we used umbrella sampling combined with the weighted histogram analysis method to calculate the potential of mean force between single-stranded DNA and hydrophobic and hydrophilic surfaces. Wang–Landau sampling and metadynamics are techniques that aim to flatten the free energy probability distribution—after a flat probability distribution is sufficiently sampled the free energy surface of the system as a function of a chosen collective variable (CV) can be reconstructed. Using metadynamics, in which a Gaussian bias is added to the free energy to discourage a simulation from visiting previously visited conformations, we have calculated the stability of DNA strands near solid surfaces with atomistic detail as well as the impact of polymer conjugation on oligonucleotide stability. The success of these advanced sampling techniques depends to a large extent on the correct choice of CV(s), which is (are) differentiable function(s) of the atoms/beads’ Cartesian coordinates; these CVs should be able to best distinguish the relevant (and physical) states the system explores along its minimum free energy pathway from one state to another (e.g., the relative orientation or distance between two macromolecules or some combination thereof). Identifying good CVs is not easy; some studies have used several simulation trials to understand the system and identify a good set of CVs while other studies have used more automated algorithms that arrive at optimal CVs for the problem at hand (for examples of both approaches see references given in refs and).
In the context of free energy calculations, one may wish to identify the minimum free energy path from one state (local free energy minimum) to another state, with these states uniquely defined in terms of the CVs. For such pursuits, finite temperature or zero temperature string methods can be used. These methods use multiple “images” of a system that trace a path along a transition from one state to another (this path is called a “string”), along some distance coordinate (α) which is related to a CV or set of CVs. The locations of the images (in phase space) are updated via differential equations of motion until the “converged” string is tangent to the gradient in the potential energy surface along its entire length, with the constraint that the images are evenly spread along α. The converged string then traces the minimum free energy path from one state to the next. Figure 8 is an example of application of the string method to find the minimum energy path between local minima in a hypothetical free energy surface. These techniques are especially powerful, for example, when one wants to probe conformational/structural transitions in biomacromolecules or defect annihilation/structural development/phase transitions in block polymers, among others.
Figure 8. Initial (yellow) and final (red) strings on a hypothetical free energy surface transitioning between two wells, V(x,y). An estimate for the minimum energy pathway is traced by interpolating between the converged (red) points. The lower plot is a zoomed-in view of the energy barrier region.
III.B. Analyzing Simulations
After the simulation is complete and an ensemble of configurations has been generated, the task turns to analyzing the molecular “trajectories” to provide useful data. It is not feasible to enumerate every possible quantity relevant to a polymer simulation, so in the section below we will describe a few interesting/useful ideas that we have applied within our own work, focusing mainly on equilibrium structural and thermodynamic quantities. We note that calculations of dynamical properties such as diffusion and transport coefficients, chain relaxation times, and viscoelastic behavior are useful outputs of simulations but, as stated earlier, outside the scope of this Perspective. We also discuss our philosophy around best practices in data reporting and analysis.
Quantification and Data Presentation
One big advantage of molecular simulation is the ability to visualize the real-space arrangement of macromolecules at length scales inaccessible to experimental microscopy techniques. Often images or movies created from simulation trajectories serve as a first check that the simulation is proceeding as intended and, after validation, help form a physical intuition about the processes at play during assembly, conformational or phase transitions, or the development of ordered structures. However, we emphasize that one cannot and should not rely solely on a simulation snapshot to support an argument, as a snapshot is a single configuration in an ensemble of configurations that a system would sample. Appropriate quantification is a necessary component of any simulation effort, even if the observation being discussed was one that was first identified visually. For example, in studies of assembly of amphiphilic copolymers, it is easy to visually classify the morphologies, e.g. spherical micelles, cylindrical micelles, and bilayers/vesicles. These visuals do not, however, present information that are experimentally measured, i.e., micelle sizes, aggregation numbers, dispersity in micelle sizes/shapes, and so forth. Therefore, in addition to qualitative visual analysis, in this example of block copolymer micelles, one should calculate structural metrics such as the micelle sizes, cluster size distribution (for quantifying dispersity), and relative shape anisotropy (for shape) to make quantitative conclusions about the structural impacts of changing block polymer characteristics. In general, it is good practice to choose appropriate analysis/quantification methods that closely parallel experimental measurements so as to facilitate validation of simulations through direct comparison to experiments; this way one can state if the model and simulation can qualitatively/semiquantitatively/quantitatively reproduce experimental behavior. Beyond facilitating a robust simulation–experiment connection, this allows simulation results to be of use to the widest possible audience.
Another necessary part of quantification is statistical analysis and error reporting. One should report error bars for all calculations so that readers can assess the variability within and between simulations. A simple yet effective strategy is to use the block averaging approach, where a simulation is split into a moderate number (∼10–30) of blocks. The mean value of a quantity of interest (e.g., Rg) is computed in each block; the average and standard deviations of the “mean” from each block are then computed, which become the ensemble average value and standard error for the entire simulation. For some quantities that experience large fluctuations or in the case of systems that exhibit nonmonomodal configurations/conformations, it may be more instructive to show the full distribution of values obtained throughout a simulation via a time series or a histogram. For example, in the case of BAB block copolymers, where B is the solvophobic block that aggregates to form micelle cores, polymer chains sample two types of configurations: one where both B blocks are part of the same micelle core and one where they are part of different cores and the A block bridges those two micelle cores. In such cases, one should report a probability distribution of Rg rather than merely an ensemble average value ± error to better represent the two populations.

Structure and Scattering
The radial distribution function, gij(r), and its Fourier-space counterpart, the partial collective structure factor Sij(q), are key analyses that characterize the structure of the system over multiple length scales as well as allow access to thermodynamic quantities of interest. The Sij(q) is compared directly with the results of scattering experiments, and analysis of the low-q (long length scale) behavior of Sij(q) provides insight about phase separation and concentration/density fluctuations. However, the range of q that can be accessed in simulations is limited to the longest real-space distance that can be reliably sampled in the simulation box, so it is often necessary to either report the lowest-q available, resort to extrapolation methods, or connect with theoretical techniques (as described in section IV.B) to describe the true low-q behavior. Sij(q) can be obtained by taking the Fourier transform of the gij(r) or calculated directly through the Debye method
In the preceding expressions, giifull(r) includes both inter- and intramolecular contributions to the pair correlation function for beads of type i, and the summations run over all beads of type i. Both calculation methods should in principle provide the same result, but performing the Fourier transform of the gij(r) (eq 3) can introduce unphysical artifacts, particularly at low q, as the Fourier transform is only rigorously defined over an infinite spatial domain, which is obviously impossible to sample in a simulation box. The Debye method (eq 4) also has some drawbacks, as it is a notoriously slow calculation that can take prohibitively long for large systems, though there have been efforts to streamline the Debye method calculation. Nonetheless, these structure factors are valuable for structural and thermodynamic analysis methods. For example, Figure 9 shows the particle–particle SPP(q) in a polymer nanocomposite at various reduced temperatures (T*), where the low-q upturn in SPP(q) at high temperatures is used as an indication of particle–particle aggregation.
Figure 9. Particle–particle structure factor for polymer-grafted nanoparticles in an explicit polymer matrix (as in Figure 5) at various reduced temperatures T* shown in the legend. The upturn at low q (right plot) at higher temperatures is an indication of particle–particle aggregation/phase separation.
Many important scattering geometries/techniques can also be calculated from simulation trajectories, including reflectivity, small-angle and wide-angle scattering, diffraction, and others. The total coherent scattering intensity from a simulation (which can be directly compared to results from small- or wide-angle scattering experiments) can be calculated as a linear combination of the partial collective structure factors.
where the summations run over all atom/bead types in the system and bi is the scattering length of species i. Equation 5 treats the atoms/beads as point scatterers, but convoluting the above expression with the form factor for a sphere (or some other model for the local distribution of scatterers) provides an additional layer of realism if needed. The above discussion focused on the time-invariant (static) structure factor, but we note that dynamic structure factors S(q,t) can also be computed to understand chain relaxation and for comparison to quasi-elastic scattering techniques.

We emphasize that a strong connection between simulation and scattering techniques is extremely powerful, as simulations aid in interpreting experimental scattering results and in designing scattering models. Furthermore, such comparisons obtained from scattering methods can help validate simulation models and methodologies. For example, in our previous work on CG simulations of blends of conjugated polymers and fullerene derivatives, we calculated a simulated diffraction technique to compare to experimental grazing-incidence X-ray diffraction patterns to lend credibility to our newly developed CG model for predicting morphologies for a variety of conjugated oligomer design parameters. In another recent study of PNCs, we found new physics decoupling the wetting/dewetting transition from the particle dispersion/aggregation transition which was confirmed experimentally via comparison of the simulation structure factors to small-angle X-ray scattering (SAXS) and small-angle neutron scattering (SANS) data. We also highlight recent work from de Pablo and co-workers that leverages an evolutionary optimization algorithm and CG simulations to reconstruct the real-space structure of block polymer thin films for nanopatterning applications from SAXS data. 
Another important structural descriptor is the intramolecular structure factor, ω(q), which is related to the polymer chain form factor P(q) by ω(q) = NP(q). The intramolecular structure factor describes the conformation of polymer chains by accounting for spatial correlations between monomers/beads within the same molecule. One way to calculate the ω(q) is using

where Ni is the number of beads of type i in a single chain/molecule, Nij = Ni + Nj if i ≠ j and Nij = Ni if i = j, and the summations are restricted to bead pairs within the same molecule. The Flory exponent, ν, which describes the scaling of the chain size with molecular weight, is related to the slope (= −1/ν) of ω(q) over the “intermediate scaling regime”, which falls between the length scale of the chain size and the length scale of the Kuhn segment (b).

One could use this technique to confirm that their polymer models can reproduce the expected scaling behavior; for ideal Gaussian chains (or real chains at theta solvent conditions) ν = 0.5, for polymers in poor solvent conditions ν ≈ 0.33, and in good solvent conditions ν ≈ 0.58. Values of ν > 0.58 indicate stretched chain conformations. In our recent work on polymer-grafted nanoparticles with comb architecture in the grafted chains, we examined the conformations of the grafted chains using this procedure to calculate the ν. As shown in Figure 10, comb-grafted polymer chains exhibited significantly stretched conformations, which increased with decreasing side chain spacing (lsc) and increasing grafting density of the polymers (Σ) on the particle. We note that care must be taken when doing this analysis, as the ν is rather sensitive to the upper and lower bounds in q used for the fitting procedure, so one should compare several different fitting procedures to evaluate the sensitivity of the reported ν values to the analysis method.

Figure 10. (a) Intermolecular structure factor for backbone beads in linear (black circles or fit 1) and comb polymer (red triangles or fit 2) chains of backbone length 20 beads grafted to a spherical nanoparticle of diameter 5d at a grafting density of 0.65 chains/d2, where d is the diameter of the polymer beads. The comb polymer has side chain length Nsc = 3 beads, and side chains are attached every lsc = 2 backbone bead. Dotted lines are linear fits to the intermediate scaling regime. (b) Flory exponent, ν, as a function of side-chain spacing, lsc, for various grafting densities of comb polymers. The ν for the corresponding linear polymers are shown with the dotted lines.

Thermodynamics and the Flory–Huggins χ
A key thermodynamic descriptor in a vast array of polymer systems is the Flory–Huggins χ parameter. There are many different definitions and methods to calculate χ, and there are some subtleties in the definition and use of the various χ parameters. In Flory’s original lattice-model treatment of polymer blends and solutions (without hydrogen bonding), the χ parameter relates to the enthalpic contribution to the free energy of mixing two unlike species as
where Ni is the length of species i and ϕi is the volume fraction of species i. If the beads have some nonbonded interaction energy wij, χ12 can be calculated as
where z is the local coordination number of a lattice site. In an off-lattice simulation, this definition can be loosely approximated using a one-fluid approach, where the total g(r) and overall density (ρ) of the fluid and the interaction potentials (Uij(r)) of the various species are used to estimate z and the wij as in eq 10. 
However, this approximation fails as the concentration fluctuations increase and the individual gij(r) depart from the total g(r) of the fluid. Another approach is to calculate an “effective” χ or χeff through the random phase approximation (RPA), as is often done in the analysis of small-angle scattering experiments. The RPA χeff can be thought of as a way to model concentration fluctuations that produce a given coherent scattering intensity: 

where Δρ is the scattering length density difference between the components, vi is the volume of one monomer of species i, and Pi(q) is the form factor of the polymer chain of type i. Strictly speaking, this RPA χeff is different from the purely enthalpic Flory–Huggins χ, as χeff takes into account entropic and packing-related effects. χeff can be used to elucidate the entropic driving force for mixing/demixing in systems where the purely “enthalpic” Flory–Huggins χ would be zero (e.g., blends of chains of the same chemistry but different architecture or chain stiffness). However, the RPA χeff are most correct in the case of isotropic, incompressible melts and well-mixed systems, limiting their applicability somewhat. Another method to calculate χ uses thermodynamic integration to progressively “morph” one species into another species. By integrating the excess free energy of mixing as the morphing occurs, the χ parameter is calculated. Irrespective of the method used to calculate χ from simulation results, we again wish to emphasize the importance/impact of the choice of pairwise interactions for unlike species. The approach used to set the unlike pairwise potentials will significantly impact the obtained χ, often necessitating iterative comparison of simulated and experimental χ parameters when developing a model and simulation methodology for a system of interest.

Locating Phase Transitions
To identify phase transitions such as the lower critical solution temperature (LCST), the order–disorder transition (ODT), or the melting curves of DNA or DNA mimics, one may need to calculate appropriate order parameters that show an identifiable signature of the transition of interest. For instance, in LCST-exhibiting polymers such as PNIPAm, the LCST is thought to be related to a decrease in polymer–solvent hydrogen bonding and a corresponding increase in polymer–polymer directional interactions, leading to polymer aggregation and phase separation. In the case of elastin-like peptide (ELP) chains tethered to another macromolecule (e.g., collagen-like peptide), the LCST-like transition of ELP is identified using a peptide hydration analysis  composed of calculating the number of polymer–water hydrogen bonds and the number of waters neighboring the polymer chains as a function of temperature. As shown in Figure 11, the hydration data are classified to be in one of two clusters, one at low temperatures and one at high temperatures, with clusters identified using a K-means clustering algorithm. The LCST-like transition can then be identified as the temperature where the fraction of the data in low-temperature cluster reaches the value of 0.5. Sometimes one may need to explore a number of possible order parameters before finding the right one; for example, we found that the obvious conformational order parameters, e.g., radius of gyration or end–to–end distances, did not show a clear signature of the LCST-like transition in atomistic simulations of short ELP (fewer than 10 repeats of VPGXG residue pentads). In other systems with LCST or first-order phase transitions, the appropriate order parameters may be density or concentration. Calculation of free energies as a function of concentration or density using advanced sampling techniques would then be needed to identify the phase transition. Therefore, depending on the problem at hand, one may need to define and calculate appropriate order parameters and use appropriate simulation techniques (see “advanced sampling” within section III.A) to reliably identify the location of a phase transition; the correct choice of order parameter may not be immediately apparent/obvious.

Figure 11. (a) Average number of peptide–water hydrogen bonds as a function of number of waters within 2.43 Å of the peptide backbone at varying temperature (temperature indicated by colorbar). (b) Same data as (a), but with all data points plotted individually for each configuration and colored by temperature. (c) Same data as (b), but with data colored by cluster as identified by the K-means algorithm. (d) Fraction of data points in the high-temperature (green upward triangles) and low-temperature (yellow downward triangles) clusters as a function of temperature; the shift in these quantities was used to identify the LCST.

Identification of the ODT is commonly done in block copolymer melts by visually distinguishing a disordered state and an ordered state and quantified through emergence of a microphase separation peak in the structure factors. In polymers with liquid crystalline entities in the backbone or side chain or semicrystalline polymers (e.g., conjugated polymers) ODT can also be identified as a point of sharp increase in local orientational order parameters like S2 or P2 which are 0 in a disordered state and tend toward 1 in an ordered state. Other avenues toward identifying phase transitions or ordering include the Binder cumulant, which can be used to locate the critical point, and the Verlet–Hansen rule for freezing/crystallization. We note that simulations that ramp a particular thermodynamic quantity (e.g., temperature, pressure, density, etc.) to locate the phase boundary can exhibit hysteresis in the transition point. In this case, or if the time scale of a structural transition is not feasible in the simulation, it may be instructive to perform free energy calculations to more rigorously identify the free energy minimum at a given thermodynamic state.

Use of Analysis Software Packages
Apart from the examples discussed in detail above, there are a host of other important widely used analysis methods of relevance to polymer systems, such as primitive path analysis to identify chain entablements, Voronoi tessellation to determine the atomic/bead packing and/or free volume, solvent-accessible surface area to examine phase or conformational transitions, the weighted histogram analysis method to calculate potentials of mean force, and so forth. For many of these complex methods, there are open-source software packages available to process the simulation trajectories and/or perform the calculations. However, when a package is used from another source, it is important to validate that the package is being applied correctly. One way to validate the user’s choice of package and calculation using the package is to reproduce previously published (correct) results using other packages/in-house codes. If one is successful in matching prior results, she/he can proceed with more confidence that the package is being used correctly. Furthermore, carefully reporting all relevant system parameters and analysis settings used is an important consideration to enable reproducibility and transparency.

IV. Linking Simulations with Experiments, Theory, and Machine Learning
IV.A. Experiments
Throughout this Perspective we have advocated for comparison between simulations and experiments whenever possible to validate the chosen model, simulation method, and/or analysis. When making such comparisons, one has to be reasonable in the expectations of qualitative/semiquantitative/quantitative agreement one should get based on the strengths/limitations of the model and simulation method at hand. When one uses CG models, one can expect to see quantitative agreement in scaling exponents or parameters that have universality and are based on solid foundations of polymer physics (e.g., exponents of polymer chain size scaling with molecular weight for polymer solutions with varying solvent quality). Besides such universal behavior, emphasizing/expecting quantitative agreement is unreasonable in most cases with heavily coarse-grained models due to the loss of chemical details. Atomistically detailed or chemically specific CG models may produce reasonable quantitative agreement with experiments in some (but not all) physical parameters. Regardless of model, there are cases where quantitative agreement should never be expected due to the selected simulation algorithm/protocol/ensemble. For example, in glassy polymer systems one may wish to calculate the glass transition temperature (Tg), which in simulations is often found via the intersection of linear fits of density versus temperature data both above and below the glass transition. To obtain the density versus temperature data, the polymer system is gradually cooled from a liquid state into the glassy state, but even the slowest cooling rate in the most carefully done simulation protocol is much higher than done in experiments (∼10 orders of magnitude faster in simulations) due to the short time scales accessible in simulations. This high simulated cooling rate is known to raise Tg significantly; thus, one cannot expect quantitative agreement in Tg between simulations and experiments without application of additional corrections for these rate effects. It is however useful and fair to examine trends in Tg from simulations for a set of design parameters (e.g., polymer architecture, chemistry, and molecular weight) and expect similar qualitative trends in experiments. Similarly, because of the flattening of free energy landscapes introduced by CG approaches, dynamics obtained via CG simulations can only qualitatively be used to compare to experiments. When comparing to experimental results, it is also important to keep in mind any limitations or uncertainties inherent to the experimental analyses—for example, instrument smearing and dispersity effects in measured scattering patterns or positional uncertainty when calculating g(r) or other structural metrics from experimental microscopy images. Judiciously reproducing these instrumental limitations in analysis of simulation results can sometimes facilitate an even more robust experimental/simulation comparison.
IV.B. Theory
Using theory in conjunction with simulations is a good way to circumvent some of simulations’ key limitations, which include (but are not limited to) finite system size effects, the inability to capture certain phenomena without specialized/advanced algorithms (e.g., large length or time scale structural/conformational transitions), the inability to capture many decades of time scales (e.g., dynamics from fast vibrations to long polymer relaxation times), and so forth. Theoretical approaches may (because of the specifics of their derivation/formalism) avoid some of these limitations, with (often) the added bonus of being extremely fast/efficient calculations. Thus, one could envision a combined simulation and theory approach where simulations at key state points feed information into or validate the theoretical approach, while the theory is used to either quickly fill in large gaps in the parameter space or provide access to time or length scales that are computationally infeasible within a simulation-only method. One may choose to complement a polymer simulation with one of the many theoretical methods used for polymer studies, such as classical density functional theory or DFT, self-consistent field theory or SCFT, polymer reference interaction site model or PRISM, statistical associating fluid theory or SAFT, and others. For brevity, in the next paragraphs we describe the key ideas behind a few of these approaches and give a few examples of the types of systems for which these theoretical methods have been useful for the study of polymers.
Classical density functional theory (classical DFT) is a technique where the free energy of the system is expressed as a function of the local density of the components; minimizing the free energy “functional” provides access to the equilibrium density distributions. Classical DFT has been used extensively for predicting structure and thermodynamics in bulk polymer systems, polymer melts, blends or polymer–nanoparticle mixtures, polymers grafted to or adsorbing near solid/patterned interfaces, polymers placed in confinement, undergoing phase transitions, and so forth (see references cited in review article). Self-consistent field theory (SCFT) is another widely used theoretical approach for polymer studies; we direct the reader to a recent perspective article that reviews the use of SCFT and presents an open source package for “broader use” of SCFT in the area of polymers. SCFT essentially reduces a complex system of multiple polymer chains interacting in a polymeric material to a simpler problem of a single chain in a field created by the other components of the system. SCFT has been particularly useful for block copolymer systems and in predicting the ODT and various ordered morphologies that a copolymer melt can access as a function of block copolymer design (sequence, chemistry, architecture) under various conditions (e.g., in the bulk, near surfaces, and under confinement). In some cases, DFT and SCFT have been combined together to study phase diagrams of morphologies within polymer nanocomposites. Another related theoretical method is the dynamic density functional theory (DDFT) method of Fraaije and co-workers, which has been applied to a large number of studies within the field of polymers; MesoDyn is the name of the commercial version of the DDFT software. In contrast to SCFT which is aimed at equilibrium structure, the diffusive equations in DDFT allow the user to capture structural evolution in inhomogeneous polymer systems. If DDFT is taken to equilibrium, it converges to the equilibrium solutions of the SCFT equations. Fredrickson and co-workers have devised improved strategies (over that of the original DDFT work) to compute saddle points, map out complex phase diagrams, and calculate free energies in polymer systems in complicated and dynamic environments, such as block polymer structure evolution during solvent evaporation. 
Although the above mean-field-based methods are computationally more efficient relative to full-fledged particle-based simulation techniques, they treat the macromolecular interactions with their surroundings through the calculated mean field. These mean-field approaches have been surprisingly accurate for dense systems of polymers (e.g., concentrated polymer solutions, molten blends, block copolymers, and polymer blends) but in some cases fail to capture experimental observations. To overcome this, some approaches combine the “particle-based” and field-based simulations together, as done in the “theoretically-informed coarse-grained” (TICG) and the “single-chain in mean-field” simulation methods applied in the studies of block copolymer melts and copolymer–nanoparticle mixtures. TICG is a particle-based MC method where the interchain interaction energy is calculated from the local densities, as in traditional field-theoretic approaches. Chain configurations calculated using single-chain MC simulations in a variety of ensembles can be used to predict quantities such as the local stresses, chemical potentials, and free energies. This method has been used to calculate coexistence curves and critical solution temperatures for binary polymer blends as well as ODTs for block copolymer systems with large-molecular-weight polymers that would be inaccessible in a purely particle-based simulation.

Figure 12. Successful use of simulations within the realm of materials discovery and eventual product design will be achieved not only by following the best practices described in this Perspective but also through links to experiments, theory, and machine learning (or “materials informatics”).

Another theoretical method that is devoid of mean-field approximations and includes density and concentration fluctuations is the integral equation polymer reference interaction site model (PRISM) theory, which we have used extensively. As shown in Figure 13a, the inputs to PRISM theory are the physical and chemical features of the model (atomistic or CG), and upon numerically solving the PRISM theory equations (eq 12) along with the appropriate “closure relations”, one obtains correlation functions which describe the liquidlike structure within the system of interest.

All terms in eq 12 are n × n matrices, with n being the number of sites used to describe the components in the system; Ĥ(q) is the Fourier-space intermolecular correlation function matrix, Ω̂(q) is the intramolecular correlation function matrix (the ωij(q) discussed in section III.B is the ij element of Ω̂(q)), and Ĉ(q) is the direct correlation function matrix. PRISM theory has been successful for elucidating structure and density/concentration fluctuations in isotropic bulk liquidlike systems of polymers (e.g., homopolymer melts, homopolymers in solution, block copolymer melts, block copolymer solutions, polymer nanocomposites, etc.) (see for example). We direct the reader to our recent papers where we introduced an open-source package for performing PRISM theory calculations, pyPRISM, and presented a detailed review of the method, PRISM theory’s successful applications, and some important limitations. In this section we give two examples of how PRISM theory can be used to complement a polymer simulation: in the first case, PRISM theory is used as a stand-alone tool before simulations are performed; in the second case, PRISM theory and MD simulations are used in an integrated approach which allows for exploration of structure and thermodynamics beyond what would be possible with simulations alone.

Figure 13. (a) Schematic representation of the inputs and outputs of a PRISM theory calculation in general and as implemented in the pyPRISM open source code. (b) Left plots are bead–bead structure factor for the solvophobic B block for three sequences of an amphiphilic AB block copolymer at varying solvophobicity (εBB) and sequences as denoted in titles of the plots. The PRISM results at low solvophobicity predict the likelihood of micro- or macrophase separation at high solvophobicity, confirmed by simulations at low and high solvophobicity (right panel). (c) Effective polymer–solvent χ parameter for a polystyrene–toluene system at varying solution concentration and varying polymer architecture; the χeff was calculated using a combined MD simulation–PRISM theory approach as described above.

In the first example (Figure 13b), PRISM theory is used to study A–B amphiphilic block copolymer solutions as a function of increasing solvophobicity of the B block for varying copolymer sequences (diblock and triblock) and composition (solvophobic-rich or solvophilic-rich). The intermolecular pair correlations and structure factors from PRISM theory at low solvophobicity values (where the solution is in a disordered state) show excellent quantitative agreement with results from MD simulations. Strikingly, even though PRISM theory failed to converge to a numerical solution at higher solvophobicities (which correspond to ordered structures in MD simulations), the results at low solvophobicity predicted many of the structural and thermodynamic signatures of block copolymer solutions at higher solvophobicities, including the clustering transition as well as the presence of micro- and/or macrophase separation at higher solvophobicities. This example demonstrates the use of PRISM theory, which is computationally much faster than MD simulations, as a useful presimulation step to explore a large design parameter space to identify systems and conditions of interest, which can then be studied in detail using computationally intensive molecular simulations or experiments.

In the second example, MD simulations and PRISM theory are used synergistically to study the structure and thermodynamics of polymer solutions. An effective polymer–solvent χ in a polystyrene–toluene solution is calculated as a function of polymer architecture (Figure 13c) by obtaining the terms in eq 12 (i.e., the Ĥ(q) and the Ω̂(q)) directly from the MD trajectories and calculating Ĉ(q) using the PRISM formalism in eq 12. Then using the terms in the Ĉ(q) matrix, the polymer–solvent χeff is calculated as

in which ϕi is the volume fraction of species i, R is the ratio of bead volumes R = vi/vj, and ĉij(0) is the i–j element of the direct correlation function matrix at zero wavenumber. This approach allows for the calculation of a χeff without the assumptions used in the RPA χeff described in section III.B.

IV.C. Machine Learning
Advances in computer science also provide opportunities for polymer scientists and engineers to take tools like machine learning, artificial intelligence/neural networks, and evolutionary optimization to predict materials properties using the large amount of data available from simulated polymer systems. This process of identifying new trends in polymer properties or new candidate polymers with desired material properties through machine learning on existing data is called “polymer informatics”. It would not be wrong to say that polymer informatics is still in its infancy as compared to the far more mature bioinformatics or inorganic materials informatics. This is partly due to fewer polymer databases, lack of consensus in naming conventions to identify databases, and the procedural/protocol-dependent nature of many calculated/measured polymer properties, among others. Some of these issues are linked to points made in this Perspective with regards to model validity, protocols to follow to ensure the simulations are reproducible, and the ways to report simulation data and associated error. Clearly, the success of polymer informatics is heavily dependent on availability of data, which in the context of polymer simulations is raw unprocessed simulation trajectories, raw data from production run of simulations, and of course well-documented model, simulation, and analysis sections in publications. We believe this necessary burden of ensuring simulation reproducibility and data sharing lies on researchers (first and foremost), publishers, and funding agencies to enable continued development of such tools/approaches.

V. Conclusion
In this Perspective, we provide the potential reader—either a new student beginning their research in the area of polymer simulations or an expert from another field that is new to simulations and wishes to use simulations as an additional tool to probe her/his systems—with a roadmap to follow when designing and performing a polymer simulation. In each section and with representative examples, we highlight some key considerations, pitfalls, and opportunities in model development, system building, equilibration, sampling, data analysis, and leveraging the connection between simulations, experiments, and theoretical methods. Throughout, we also discuss our ideas around best practices in research design, data analysis, and data/error reporting to achieve transparent, reproducible, and reliable simulations. In this way, we wish to ensure that polymer modeling and simulations continue to be a valuable, reliable, trustworthy, and impactful tool of use to the wider polymer science community.

